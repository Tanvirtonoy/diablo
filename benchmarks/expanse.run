#!/bin/bash
#SBATCH -A uot166
#SBATCH --job-name="diablo"
#SBATCH --output="run.log"
#SBATCH --partition=compute
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=128
#SBATCH --mem=248G
#SBATCH --export=ALL
#SBATCH --time=60    # time limit in minutes

nodes=$SLURM_NNODES
echo "Number of nodes = " $nodes

# Expanse node: 128 cores (126 available), 256 GB RAM
#   executor-cores = 12   (10 executors/node)
#   executor-memory = 24GB
#   num-executors = nodes*10-1
executors=$((nodes*10-1))
echo "Number of executors = " $executors

SPARK_OPTIONS="--driver-memory 24G --num-executors $executors --executor-cores 12 --executor-memory 24G --conf spark.executor.extraJavaOptions=-Xss512m --driver-java-options '-Xss512m' --supervise"

export HADOOP_CONF_DIR=$HOME/expansecluster
module load openjdk

SW=/expanse/lustre/projects/uot166/fegaras

export DIABLO_HOME=$SW/diablo
export SCALA_HOME=$SW/scala-2.12.3
export HADOOP_HOME=$SW/hadoop-3.2.2
export MYHADOOP_HOME=$SW/myhadoop
export SPARK_HOME=$SW/spark-3.1.2-bin-hadoop3.2

PATH="$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$MYHADOOP_HOME/bin:$SCALA_HOME/bin:$PATH"

# location of scratch space
scratch=/scratch/$USER/job_$SLURM_JOB_ID
export SPARK_JAVA_OPTS="-Dspark.local.dir=$scratch"
export MH_SCRATCH_DIR=$scratch

myhadoop-configure.sh -s $scratch
echo "export TMP=$scratch" >> $HADOOP_CONF_DIR/spark/spark-env.sh
echo "export TMPDIR=$scratch" >> $HADOOP_CONF_DIR/spark/spark-env.sh
echo "export SPARK_MASTER_HOST=$SPARK_MASTER_IP" >> $HADOOP_CONF_DIR/spark/spark-env.sh
export SPARK_MASTER_HOST=$SPARK_MASTER_IP
source $HADOOP_CONF_DIR/spark/spark-env.sh

export SPARK_LOCAL_DIRS=$scratch
export TMPDIR=$scratch/tmp
export _JAVA_OPTIONS=-Djava.io.tmpdir=$TMPDIR

# start HDFS
$HADOOP_HOME/sbin/start-dfs.sh
# start Spark
$SPARK_HOME/sbin/start-all.sh -h $SPARK_MASTER_HOST

JARS=.
for I in `ls $SPARK_HOME/jars/*.jar -I *unsafe*`; do
    JARS=$JARS:$I
done

scale=100        # scale of datasets
ns=5             # number of datasets per experiment
repeat=4         # number of repetitions of each experiment

for ((i=1; i<=$ns; i++)); do   # for each different dataset

   j=$(echo "scale=3;sqrt($i/$ns)*400*$scale" | bc); n=${j%.*}
      spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Add --master $MASTER $SPARK_OPTIONS test.jar $repeat $n

   j=$(echo "scale=3;sqrt($i/$ns)*80*$scale" | bc); n=${j%.*}
      spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Multiply --master $MASTER $SPARK_OPTIONS test.jar $repeat $n

   j=$(echo "scale=3;sqrt($i/$ns)*200*$scale" | bc); n=${j%.*}
      spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Multiply --master $MASTER $SPARK_OPTIONS test.jar $repeat $n

   j=$(echo "scale=3;sqrt($i/$ns)*200*$scale" | bc); n=${j%.*}
      spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Factorization --master $MASTER $SPARK_OPTIONS test.jar $repeat $n $n 10

   j=$(echo "scale=3;sqrt($i/$ns)*200*$scale" | bc); n=${j%.*}
      spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Factorization --master $MASTER $SPARK_OPTIONS test.jar $repeat $n $n 10

done

spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Add --master $MASTER $SPARK_OPTIONS test.jar 2 12000
spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Multiply --master $MASTER $SPARK_OPTIONS test.jar 2 10000 1000

hdfs dfs -mkdir -p /user/$USER
hdfs dfs -put page-rank2.txt /user/$USER/page-rank.txt
spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class PageRank --master $MASTER $SPARK_OPTIONS test.jar 2 20000 page-rank2.txt

spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class Factorization --master $MASTER $SPARK_OPTIONS test.jar 2 20000 20000 10

hdfs dfs -put data_10000/lr_input1.txt /user/$USER/lr_input1.txt
hdfs dfs -put data_10000/lr_output1.txt /user/$USER/lr_output1.txt
hdfs dfs -put data_10000/lr_input2.txt /user/$USER/lr_input2.txt
hdfs dfs -put data_10000/lr_output2.txt /user/$USER/lr_output2.txt
spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class LinearRegression --master $MASTER $SPARK_OPTIONS test.jar 1000 10 /user/$USER/lr_input1.txt /user/$USER/lr_output1.txt /user/$USER/lr_input2.txt /user/$USER/lr_output2.txt

hdfs dfs -put X_train.txt /user/$USER/X_train.txt
hdfs dfs -put y_train.txt /user/$USER/y_train.txt
hdfs dfs -put X_test.txt /user/$USER/X_test.txt
hdfs dfs -put y_test.txt /user/$USER/y_test.txt
spark-submit --jars ${DIABLO_HOME}/lib/diablo.jar --class NeuralNetwork --master $MASTER $SPARK_OPTIONS test.jar 10000 10 /user/$USER/X_train.txt /user/$USER/y_train.txt /user/$USER/X_test.txt /user/$USER/y_test.txt

$SPARK_HOME/sbin/stop-all.sh
$HADOOP_HOME/sbin/stop-dfs.sh
$MYHADOOP_HOME/bin/myhadoop-cleanup.sh
